from collections import namedtuple
from network import Network
from environment import UltimateTTT
from player import MCTSPlayer, RandomPlayer, NNPlayer
from policy import NNPolicy, RandomPolicy
from tqdm import tqdm
import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import time
import random

'''
given a history list and the winner of the game
output a list of named tuple that is useful for training the neural network
'''
def to_dataset(history, winner):
    Data = namedtuple('data',['board', 'probs', 'score'])
    dataset = []
    for state, probs in history:
        if not state['game_end']:
            relative_board = state['inner']*state['current']
            score = 0 if winner == 2 else winner*state['current']
            data = Data(relative_board, probs, score)
            dataset.append(data)
    
    return dataset


'''
let the player play game with itself
input a player and its copy
output the dataset generated by the game
'''
def self_play(player1:MCTSPlayer,player2:MCTSPlayer):
    game = UltimateTTT(player1,player2)
    game.play()
    hist1 = player1.get_history()
    hist2 = player2.get_history()
    winner = game.winner
    
    data1 = to_dataset(hist1,winner)
    data2 = to_dataset(hist2,winner)
    dataset = data1 + data2

    # shuffle the dataset
    random.shuffle(dataset)

    states = []
    probs = []
    scores = []
    for data in dataset:
        states.append([data.board])
        probs.append(data.probs)
        scores.append(data.score)
    
    states = np.array(states)
    probs = np.array(probs)
    scores = np.array(scores)

    
    return torch.tensor(states).float(), torch.tensor(probs), torch.tensor(scores)

def loss_function(z,v,pi,p):
    m = z.size()[0]
    val_loss = torch.sum(torch.square(z - v))
    pol_loss = torch.sum(pi*torch.log(p))
    loss = (val_loss - pol_loss)/m
    return loss


def train(num_self_play = 10, num_epoch = 10):
    net = Network()
    optimizer = optim.Adam(net.parameters(), lr=0.0001)
    pol = NNPolicy(net)
    sim = NNPlayer(net)
    player1 = MCTSPlayer(pol, sim, 200, True)
    player2 = MCTSPlayer(pol,sim, 200, True)
    losses = []
    for _ in tqdm(range(5)):
        
        states,pi,z = self_play(player1, player2)
        for _ in range(10):
            p,v = net(states)
            optimizer.zero_grad()
            loss = loss_function(z,v,pi,p)
            losses.append(loss.item())
            loss.backward()
            optimizer.step()

        player1.reset()
        player2.reset()

    plt.plot(range(1,len(losses)+1), losses)
    plt.show()

    
    # torch.save(net,'model.pt')
    # try:
    #     model = torch.load('modl.pt')
    # except FileNotFoundError:
    #     print('file not found')
    #     model = Network()
    # print(model)

    





def main():
    # net = Network()
    # pol = RandomPolicy()
    # sim = RandomPlayer()
    # player1 = MCTSPlayer(pol, sim, store_hist=True)
    # player2 = MCTSPlayer(pol, sim, store_hist=True)
    # state, prob,score = self_play(player1,player2)
    # print(state.size())
    # print(prob.size())
    # print(score.size())
    train()

    # start = time.time()
    # game.play()
    # print(time.time() - start)

if __name__ == '__main__':
    main()