from collections import namedtuple
from network import Network
from environment import UltimateTTT
from player import MCTSPlayer, NNPlayer
from policy import NNPolicy
from tqdm import tqdm
from termcolor import colored
import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import pickle
import random

'''
given a history list and the winner of the game
output a list of named tuple that is useful for training the neural network
'''
def to_dataset(history, winner):
    Data = namedtuple('data',['board', 'probs', 'score'])
    dataset = []
    for state, probs in history:
        if not state['game_end']:
            relative_board = state['inner']*state['current']
            score = 0 if winner == 2 else winner*state['current']
            data = Data(relative_board, probs, score)
            dataset.append(data)
    
    return dataset

'''
Given a batch dataset and a size
Output a list of mini-batches specified by the mini-batch size
'''
def to_mini_batch(dataset, mini_size = 10):
    batch = []
    dim = dataset.size()[0]
    for index in range(0,dim,mini_size):
        batch.append(dataset[index:index+mini_size])
    
    return batch




'''
let the player play game with itself
input a player and its copy
output the dataset generated by the game
'''
def self_play(player1:MCTSPlayer,player2:MCTSPlayer):
    game = UltimateTTT(player1,player2)
    game.play()
    hist1 = player1.get_history()
    hist2 = player2.get_history()
    winner = game.winner
    
    data1 = to_dataset(hist1,winner)
    data2 = to_dataset(hist2,winner)
    dataset = data1 + data2

    # shuffle the dataset
    random.shuffle(dataset)

    states = []
    probs = []
    scores = []
    for data in dataset:
        states.append([data.board])
        probs.append(data.probs)
        scores.append(data.score)
    
    states = np.array(states)
    probs = np.array(probs)
    scores = np.array(scores)

    
    return torch.tensor(states).float(), torch.tensor(probs), torch.tensor(scores)

'''
squared loss of values plus cross-entropy loss of move probabilities
'''
def loss_function(z,v,pi,p):
    m = z.size()[0]
    val_loss = torch.sum(torch.square(z - v))
    pol_loss = torch.sum(pi*torch.log(p))
    loss = (val_loss - pol_loss)/m
    return loss

'''
Given a current best player, a baseline player and the number of games to play
Output a normalized score in [-1,1] for the curent best player based on the game results
'''
def eval(current_best:MCTSPlayer, baseline:MCTSPlayer, num_games = 20):
    score = 0
    print("Evaluation in progress:")
    for i in tqdm(range(num_games)):
        # alternating x and o
        if i % 2 == 0:
            game = UltimateTTT(current_best, baseline)
            game.play()
            final_state = game.get_state()
            if final_state['winner'] == 1:
                score +=1
            elif final_state['winner'] == -1:
                score -= 1
        else:
            game = UltimateTTT(baseline, current_best)
            game.play()
            final_state = game.get_state()
            if final_state['winner'] == -1:
                score +=1
            elif final_state['winner'] == 1:
                score -= 1
        
        # reset both players
        current_best.reset()
        baseline.reset()
    
    return score/num_games

            

'''
num_self_play: number of self-playing games
num_epoch: number of epoch for each batch of data
mini_size: size of mini-batch
lr: learning rate
checkpoint: number of runs per save
start: starting number of model to continue
'''
def train(num_self_play = 100, num_epoch = 10, mini_size = 10, lr = 1e-4, checkpoint = 5, start = None):

    model = None
    total_loss = None

    # check if the starting point is specified
    if start is None:
        print('Training the network from the beginning.')
        model = Network()
        start = 0
    else: 
        model = torch.load(f'./models/model_{start}.pt')
        print(colored(f'Model {start} successfully loaded.', 'green'))
    

    # load the loss record
    try:
        with open('loss.txt','rb') as fp:
            total_loss = pickle.load(fp)
            print(colored('Loss record successfully loaded.','green'))
    except FileNotFoundError:
        print('Cannot find loss record file. Start from the beginning.')
        total_loss = []
    
    # check if picking up the latest
    if start != len(total_loss):
        input(colored('Warning:starting point and the number of losses are not equal! Press ENTER if continues anyway.', 'red'))


    optimizer = optim.Adam(model.parameters(),lr)
    # creating player and its copy
    pol = NNPolicy(model)
    sim = NNPlayer(model)
    player = MCTSPlayer(pol, sim, store_hist=True)
    player_cpy = MCTSPlayer(pol, sim, store_hist=True)

    print(colored('Start self-playing process:', 'green'))
    for index in tqdm(range(start, start + num_self_play)):
        # saving the model and loss
        if index % checkpoint == 0:
            with open('loss.txt', 'wb') as fp:
                pickle.dump(total_loss,fp)    
            torch.save(model,f'./models/model_{index}.pt')

        # get batch and split into mini-batches    
        batch_state, batch_pi, batch_z = self_play(player, player_cpy)
        mini_states = to_mini_batch(batch_state, mini_size)
        mini_pi = to_mini_batch(batch_pi)
        mini_z = to_mini_batch(batch_z)

        # training the network
        for epoch in range(num_epoch):
            for state, pi, z in zip(mini_states, mini_pi, mini_z):
                p,v = model(state)
                optimizer.zero_grad()
                loss = loss_function(z,v,pi,p)
                loss.backward()
                optimizer.step()
        
        # compute the mean loss for the entire batch
        batch_p, batch_v = model(batch_state)
        mean_loss = loss_function(batch_z, batch_v, batch_pi, batch_p)
        total_loss.append(mean_loss)

        # resst players
        player.reset()
        player_cpy.reset()
    
    # saving the model and the loss record at the end
    with open('loss.txt', 'wb') as fp:
        pickle.dump(total_loss,fp)    
    torch.save(model,f'./models/model_{start + num_self_play}.pt')


def main():
    train(100, start=15)


if __name__ == '__main__':
    main()