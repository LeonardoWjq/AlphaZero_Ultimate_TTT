from collections import namedtuple
from network import Network
from environment import UltimateTTT
from player import MCTSPlayer, RandomPlayer, NNPlayer
from policy import NNPolicy, RandomPolicy
from tqdm import tqdm
import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import time
import random

'''
given a history list and the winner of the game
output a list of named tuple that is useful for training the neural network
'''
def to_dataset(history, winner):
    Data = namedtuple('data',['board', 'probs', 'score'])
    dataset = []
    for state, probs in history:
        if not state['game_end']:
            relative_board = state['inner']*state['current']
            score = 0 if winner == 2 else winner*state['current']
            data = Data(relative_board, probs, score)
            dataset.append(data)
    
    return dataset

def to_mini_batch(dataset, mini_size = 10):
    batch = []
    dim = dataset.size()[0]
    for index in range(0,dim,mini_size):
        batch.append(dataset[index:index+mini_size])
    
    return batch




'''
let the player play game with itself
input a player and its copy
output the dataset generated by the game
'''
def self_play(player1:MCTSPlayer,player2:MCTSPlayer):
    game = UltimateTTT(player1,player2)
    game.play()
    hist1 = player1.get_history()
    hist2 = player2.get_history()
    winner = game.winner
    
    data1 = to_dataset(hist1,winner)
    data2 = to_dataset(hist2,winner)
    dataset = data1 + data2

    # shuffle the dataset
    random.shuffle(dataset)

    states = []
    probs = []
    scores = []
    for data in dataset:
        states.append([data.board])
        probs.append(data.probs)
        scores.append(data.score)
    
    states = np.array(states)
    probs = np.array(probs)
    scores = np.array(scores)

    
    return torch.tensor(states).float(), torch.tensor(probs), torch.tensor(scores)

def loss_function(z,v,pi,p):
    m = z.size()[0]
    val_loss = torch.sum(torch.square(z - v))
    pol_loss = torch.sum(pi*torch.log(p))
    loss = (val_loss - pol_loss)/m
    return loss

def eval(current_best:MCTSPlayer, baseline:MCTSPlayer, num_games = 20):
    score = 0
    for i in num_games:
        # alternating x and o
        if i % 2 == 0:
            game = UltimateTTT(current_best, baseline)
            game.play()
            final_state = game.get_state()
            if final_state['winner'] == 1:
                score +=1
            elif final_state['winner'] == -1:
                score -= 1
        else:
            game = UltimateTTT(baseline, current_best)
            game.play()
            final_state = game.get_state()
            if final_state['winner'] == -1:
                score +=1
            elif final_state['winner'] == 1:
                score -= 1
        
        current_best.reset()
        baseline.reset()
    
    return score/num_games

            


def train(num_self_play = 10, num_epoch = 10):
    net = Network()
    optimizer = optim.Adam(net.parameters(), lr=0.0001)
    pol = NNPolicy(net)
    sim = NNPlayer(net)
    player1 = MCTSPlayer(pol, sim, 200, True)
    player2 = MCTSPlayer(pol,sim, 200, True)
    losses = []
    for _ in tqdm(range(5)):
        
        states,pi,z = self_play(player1, player2)

        states = to_mini_batch(states)
        pi = to_mini_batch(pi)
        z = to_mini_batch(z)
        print(z)
        for epoch in range(10):
            for mini_state, mini_pi, mini_z in zip(states, pi, z):
                p,v = net(mini_state)
                optimizer.zero_grad()
                loss = loss_function(mini_z,v,mini_pi,p)
                losses.append(loss.item())
                loss.backward()
                optimizer.step()

        player1.reset()
        player2.reset()
    
    torch.save(net, 'model.pt')
    plt.plot(range(1,len(losses)+1), losses)
    plt.show()

    
    # torch.save(net,'model.pt')
    # try:
    #     model = torch.load('modl.pt')
    # except FileNotFoundError:
    #     print('file not found')
    #     model = Network()
    # print(model)

    





def main():
    # net = Network()
    # pol = RandomPolicy()
    # sim = RandomPlayer()
    # player1 = MCTSPlayer(pol, sim, store_hist=True)
    # player2 = MCTSPlayer(pol, sim, store_hist=True)
    # state, prob,score = self_play(player1,player2)
    # print(state.size())
    # print(prob.size())
    # print(score.size())
    train()

    # start = time.time()
    # game.play()
    # print(time.time() - start)

if __name__ == '__main__':
    main()